{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91544cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2021-10-08\n",
    "# GitHub: https://github.com/jaaack-wang \n",
    "# About: Practical Tips for Projects for Stanford CS224N- NLP with Deep Learning | Winter 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd06afa",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [1. Final project](#1)\n",
    "    - [1.1 Default](#1-1)\n",
    "    - [1.2 Two basic starting points of finding research topics](#1-2)\n",
    "    - [1.3 Project types](#1-3)\n",
    "    - [1.4 Interesting places to start](#1-4)\n",
    "    - [1.5 Must-haves](#1-5)\n",
    "    - [1.6 Where to find data](#1-6)\n",
    "- [2. RNNs recap](#2)\n",
    "    - [2.1 Recap of RNN](#2-1)\n",
    "    - [2.2 Gated Recurrent Unit Recap](#2-2)\n",
    "    - [2.3 Compare ungated and gated Unit](#2-3)\n",
    "    - [2.4 Two most important variants of gated RNN](#2-4)\n",
    "    - [2.5 LSTM](#2-5)\n",
    "- [3. MT](#3)\n",
    "    - [3.1 Problems](#3-1)\n",
    "    - [3.2 Solutions](#3-2)\n",
    "    - [3.3 MT evaluation](#3-3)\n",
    "    - [3.4 BLEU](#3-4)\n",
    "- [4. Steps of Working on a project](#4)\n",
    "- [5. References](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48d26a",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1. Final project\n",
    "\n",
    "<a name='1-1'></a>\n",
    "## 1.1 Default\n",
    "\n",
    "Task: Building a textual question answering system for SQuAD\n",
    "- Stanford Question Answering Dataset 2.0\n",
    "    - https://rajpurkar.github.io/SQuAD-explorer/\n",
    "\n",
    "\n",
    "<a name='1-2'></a>\n",
    "## 1.2 Two basic starting points of finding research topics\n",
    "\n",
    "- \\[Nails\\]: Start with **a (domain) problem of interest** and try to find good/better ways to address it than are currently known/used\n",
    "- \\[Hammers\\]: Start with **a technical approach of interest**, and work out good ways to extend or improve it or new ways to apply it\n",
    "\n",
    "\n",
    "<a name='1-3'></a>\n",
    "## 1.3 Project types\n",
    "\n",
    "This is not an exhaustive list, but most projects are one of\n",
    "\n",
    "1. Find an **application/task** of interest and explore how to approach/solve it effectively, usually applying an existing neural network model\n",
    "2. Implement a complex neural architecture and demonstrate its **performance on some data**\n",
    "3. Come up with **a new or variant neural network model** and explore its **empirical success**\n",
    "4. **Analysis project**. Analyze the behavior of a model: how it represents linguistic knowledge or what kinds of phenomena it can handle or errors that it makes\n",
    "5. **Rare theoretical project**: Show some interesting, non-trivial properties of a model type, data, or a data representation\n",
    "\n",
    "\n",
    "<a name='1-4'></a>\n",
    "## 1.4 Interesting places to start\n",
    "\n",
    "- Look at ACL anthology for NLP papers:\n",
    "    - https://aclanthology.info\n",
    "- Also look at the online proceedings of major ML conferences: \n",
    "    - NeurIPS, ICML, ICLR\n",
    "- Look at past cs224n project \n",
    "    - See the class website\n",
    "- Look at online preprint servers, especially: \n",
    "    - https://arxiv.org\n",
    "- Even better: look for an interesting problem in the world\n",
    "\n",
    "- Leaderboards (SODA): https://paperswithcode.com/sota (Not always correct, though)\n",
    "\n",
    "\n",
    "<a name='1-5'></a>\n",
    "## 1.5 Must-haves\n",
    "\n",
    "- Suitable data\n",
    "    - Usually aiming at: 10,000+ labeled examples by milestone\n",
    "- Feasible task\n",
    "- Automatic evaluation metric\n",
    "- NLP is central to the project\n",
    "\n",
    "\n",
    "<a name='1-6'></a>\n",
    "## 1.6 Where to find data\n",
    "\n",
    "- Linguistic Data Consortium (https://catalog.ldc.upenn.edu/)\n",
    "- For machine translation: http://statmt.org\n",
    "- For dependency parsing: https://universaldependencies.org/\n",
    "- List of datasets:\n",
    "    - https://machinelearningmastery.com/datasets-natural-language-processing/\n",
    "    - https://github.com/niderhoff/nlp-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeab369",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2. RNNs recap\n",
    "\n",
    "<a name='2-1'></a>\n",
    "## 2.1 Recap of RNN\n",
    "\n",
    "- Also see the lecture 7 notes.\n",
    "- How it works:\n",
    "<img src='../images/9-RNNRecap.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "- Vanishing gradient problem:\n",
    "<img src='../images/9-RNNRecap2.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='2-2'></a>\n",
    "## 2.2 Gated Recurrent Unit Recap\n",
    "\n",
    "To prune unnecessary connections adaptively to prevent errors caused by vanishing gradient being backprogated through all the intermediate nodes (by introduction of reset/forget gate):\n",
    "\n",
    "<img src='../images/9-GatedRecurUnit.png' width='600' height='300'>\n",
    "\n",
    "<a name='2-3'></a>\n",
    "## 2.3 Compare ungated and gated Unit\n",
    "\n",
    "- Ungated\n",
    "<img src='../images/9-GatedRecurUnit2.png' width='600' height='300'>\n",
    "\n",
    "- Gated\n",
    "<img src='../images/9-GatedRecurUnit3.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='2-4'></a>\n",
    "## 2.4 Two most important variants of gated RNN\n",
    "\n",
    "<img src='../images/9-GatedRecurUnit4.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='2-5'></a>\n",
    "## 2.5 LSTM\n",
    "\n",
    "\n",
    "<img src='../images/9-LSTM.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/9-LSTM2.png' width='600' height='300'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97258201",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# 3. MT\n",
    "\n",
    "<a name='3-1'></a>\n",
    "## 3.1 Problems\n",
    "\n",
    "- Large output vocab problem for NMT, which is expensive to compute for softmax\n",
    "- Word generation problem: vocab size is limited, say, 50K\n",
    "\n",
    "<a name='3-2'></a>\n",
    "## 3.2 Solutions\n",
    "\n",
    "- Check lecture2 notes for Hierachical softmax + Noise-contrastive estimation \n",
    "- Check lecture7 for using attention\n",
    "\n",
    "<img src='../images/9-solution.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='3-3'></a>\n",
    "## 3.3 MT evaluation\n",
    "\n",
    "\n",
    "- Manual:\n",
    "    - Adequacy and Fluency (5 or 7 points cales)\n",
    "    - Error categorization\n",
    "    - Comparative ranking of translations\n",
    "- Testing in an application that uses MT as one sub-component \n",
    "    - E.g.,questionansweringfromforeignlanguagedocuments\n",
    "        - May not test many aspects of the translation (e.g., cross-lingual IR)\n",
    "- Automatic metric:\n",
    "    - BLEU(BilingualEvaluationUnderstudy) \n",
    "    - OtherslikeTER,METEOR,...\n",
    "    \n",
    "- Coming up with automatic MT evaluations has become its own research field\n",
    "    - There are many proposals:TER, METEOR, MaxSim, SEPIA, RTE-MT\n",
    "    - TERpA is a representative good one that handles some word choice variation.\n",
    "    - MT research requires some automatic metric to allow a rapid development and evaluation cycle.    \n",
    "\n",
    "    \n",
    "<a name='3-4'></a>\n",
    "## 3.4 BLEU \n",
    "\n",
    "- Reference: [Papineni et al, 2002. BLEU: a Method for Automatic Evaluation of Machine Translation.](https://aclanthology.org/P02-1040.pdf)\n",
    "- Check lecture8 for BLEU \n",
    "- Basically, the correlation between BLEU and human judgments of quality is not a strong one. \n",
    "- MT BLEU scores now approach those of human translations but their true quality remains far below human translations\n",
    "\n",
    "\n",
    "<img src='../images/9-BLEU.png' width='600' height='300'>\n",
    "<img src='../images/9-BLEU2.png' width='600' height='300'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414b574",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "# 4. Steps of Working on a project \n",
    "\n",
    "<img src='../images/9-steps.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/9-steps2.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/9-steps3.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/9-steps4.png' width='600' height='300'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cff5c",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "# 5. References\n",
    "\n",
    "- [Course website](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html)\n",
    "\n",
    "- [Lecture video](https://www.youtube.com/watch?v=fyqm8fRDgl0) \n",
    "\n",
    "- [Lecture slide](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture09-final-projects.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
