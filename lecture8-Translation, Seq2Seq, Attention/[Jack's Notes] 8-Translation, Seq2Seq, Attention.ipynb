{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91544cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2021-10-08\n",
    "# GitHub: https://github.com/jaaack-wang \n",
    "# About: Translation, Seq2Seq, Attention for Stanford CS224N- NLP with Deep Learning | Winter 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd06afa",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [1. Pre-Neural Machine Translation](#1)\n",
    "    - [1.1 Problem defined](#1-1)\n",
    "    - [1.2 Early 50s: rule-based](#1-1)\n",
    "    - [1.3 1990s-2010s: statistical](#1-1)\n",
    "- [2. Neural Machine Translation](#2)\n",
    "    - [2.1 Problem defined](#2-1)\n",
    "    - [2.2 Seq2seq Model](#2-2)\n",
    "    - [2.3 Training](#2-3)\n",
    "    - [2.4 Decoing](#2-4)\n",
    "        - [2.4.1 Greedy decoding](#2-4-1)\n",
    "        - [2.4.2 Exhaustive search decoding](#2-4-1)\n",
    "        - [2.4.3 Beam search decoding](#2-4-1)\n",
    "    - [2.5 Tradeoff of NMT](#2-5)\n",
    "        - [2.5.1 Advantages](#2-5-1)\n",
    "        - [2.5.2 Disadvantages](#2-5-2)\n",
    "    - [2.6 Eluvation: BLEU](#2-6)\n",
    "- [3. Attention](#3)\n",
    "    - [3.1 Background](#3-1)\n",
    "    - [3.2 Graphic represenation](#3-2)\n",
    "    - [3.3 Equations](#3-3)\n",
    "    - [3.4 Benifits](#3-4)\n",
    "    - [3.5 Attention as a general DL technique](#3-5)\n",
    "    - [3.6 Remaining problems](#3-6)\n",
    "    - [ 3.7 Trend](#3-7)\n",
    "- [4. References](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48d26a",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1. Pre-Neural Machine Translation\n",
    "\n",
    "<a name='1-1'></a>\n",
    "## 1.1 Problem defined\n",
    "\n",
    "**Machine Translation (MT)** is the task of translating a sentence x from one language (the _source language_) to a sentence y in another language (the _target language_).\n",
    "\n",
    "\n",
    "<a name='1-2'></a>\n",
    "## 1.2 Early 50s: rule-based\n",
    "\n",
    "Machine Translation research began in the early 1950s. Mostly, Russian → English (cold war)\n",
    "\n",
    "Systems were mostly rule-based, using a bilingual dictionary to map Russian words to their English counterparts\n",
    "\n",
    "<a name='1-3'></a>\n",
    "## 1.3 1990s-2010s: statistical\n",
    "\n",
    "- Translation model + language model (see the last lecture for LM)\n",
    "- Parallel data is needed for building the translation model\n",
    "- Parallel data breaks down --> words alignments\n",
    "- **Problem**: some words have no counterparts\n",
    "<img src='../images/8-statMTDesc.png' width='600' height='300'>\n",
    "\n",
    "- Examples from: “The Mathematics of Statistical Machine Translation: Parameter Estimation\", Brown et al, 1993. http://www.aclweb.org/anthology/J93-2003\n",
    "\n",
    "<img src='../images/8-statMTAlignment.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/8-statMTAlignment2.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/8-statMTAlignment3.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/8-statMTAlignment4.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/8-statMTAlignment5.png' width='600' height='300'>\n",
    "\n",
    "We learn   as a combination of many factors, including: \n",
    "\n",
    "- Probability of particular words aligning (also depends on position in sent)\n",
    "- Probability of particular words having particular fertility (number of corresponding words)\n",
    "\n",
    "<img src='../images/8-SMTDecoding.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/8-SMT.png' width='600' height='300'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeab369",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2. Neural Machine Translation\n",
    "\n",
    "<a name='2-1'></a>\n",
    "## 2.1 Problem defined\n",
    "\n",
    "- **Neural Machine Translation (NMT)** is a way to do Machine Translation with a single neural network\n",
    "- The neural network architecture is called **sequence-to-sequence (aka seq2seq)** and it involves two RNNs.\n",
    "\n",
    "\n",
    "<a name='2-2'></a>\n",
    "## 2.2 Seq2seq Model\n",
    "\n",
    "- Language model (Conditional)\n",
    "\n",
    "<img src='../images/8-seq2seqModel.png' width='600' height='300'>\n",
    "<img src='../images/8-seq2seqModel2.png' width='600' height='300'>\n",
    "\n",
    "- Other applications:\n",
    "    - Summarization (long text → short text)\n",
    "    - Dialogue (previous utterances → next utterance)\n",
    "    - Parsing (input text → output parse as sequence)\n",
    "    - Code generation (natural language → Python code)\n",
    "\n",
    "\n",
    "<a name='2-3'></a>\n",
    "## 2.3 Training \n",
    "\n",
    "<img src='../images/8-NNMTtraining.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='2-4'></a>\n",
    "## 2.4 Decoing\n",
    "\n",
    "<a name='2-4-1'></a>\n",
    "### 2.4.1 Greedy decoding\n",
    "- **Greedy decoding**: generate (or “decode”) the target sentence by taking argmax on each step of the decoder. Problem: Greedy decoding has no way to undo decisions!\n",
    "\n",
    "<a name='2-4-2'></a>\n",
    "### 2.4.2 Exhaustive search decoding\n",
    "- **Exhaustive search decoding**:find all possible translations and choose the one that has the highest probablity. Problem: too expensive to do exhaustively! The complexity = $O(V^T)$ where V is the vocab size and T is the length of squence/time step to translate. \n",
    "\n",
    "\n",
    "<a name='2-4-3'></a>\n",
    "### 2.4.3 Beam search decoding\n",
    "- **Beam search decoding**:On each step of decoder, keep track of the k most probable partial translations (which we call hypotheses). k is the beam size (in practice around 5 to 10). Problem: not guaranteed to find optimal solution, but much more efficient and practical than exhaustive search!\n",
    "\n",
    "\n",
    "<img src='../images/8-beamSearch.png' width='600' height='300'>\n",
    "\n",
    "Stopping condition:\n",
    "<img src='../images/8-beamSearchStoppingCondition.png' width='600' height='300'>\n",
    "\n",
    "- Problem:\n",
    "<img src='../images/8-beamSearchProblem.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='2-5'></a>\n",
    "## 2.5 Tradeoff of NMT\n",
    "\n",
    "<a name='2-5-1'></a>\n",
    "### 2.5.1 Advantages\n",
    "\n",
    "Compared to SMT, NMT has many advantages:\n",
    "- Better performance \n",
    "    - More fluent\n",
    "    - Better use of context\n",
    "    - Better use of phrase similarities\n",
    "\n",
    "- A single neural network to be optimized end-to-end \n",
    "    - No subcomponents to be individually optimized\n",
    "\n",
    "- Requires much less human engineering effort\n",
    "    - No feature engineering\n",
    "    - Same method for all language pairs\n",
    "\n",
    "\n",
    "<a name='2-5-2'></a>\n",
    "### 2.5.2 Disadvantages\n",
    "\n",
    "Compared to SMT:\n",
    "- NMT is less interpretable \n",
    "    - Hard to debug\n",
    "\n",
    "- NMT is difficult to control\n",
    "    - For example, can’t easily specify rules or guidelines for translation\n",
    "    - Safety concerns!\n",
    "    \n",
    "    \n",
    "<a name='2-6'></a>\n",
    "## 2.6 Eluvation: BLEU\n",
    "\n",
    "- Reference: [Papineni et al, 2002. BLEU: a Method for Automatic Evaluation of Machine Translation.](https://aclanthology.org/P02-1040.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849fa3f",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# 3. Attention\n",
    "\n",
    "<a name='3-1'></a>\n",
    "## 3.1 Background\n",
    "\n",
    "Encoding cannot capture all important about the source sentence (see the last lecture where the vanishing gradient problem of RNN is introduced)\n",
    "\n",
    "<img src='../images/8-seq2seqbottleneck.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "- **Attention** provides a solution to the bottleneck problem.\n",
    "- **Core idea**: on each step of the decoder, use **direct connection to the encoder** to **focus on a particular part** of the source sequence\n",
    "\n",
    "\n",
    "<a name='3-2'></a>\n",
    "## 3.2 Graphic represenation\n",
    "\n",
    "Sometimes we take the attention output from the previous step, and also feed it into the decoder (along with the usual decoder input).\n",
    "\n",
    "<img src='../images/8-attention.png' width='600' height='300'>\n",
    "<img src='../images/8-attention2.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "\n",
    "<a name='3-3'></a>\n",
    "## 3.3 Equations\n",
    "\n",
    "<img src='../images/8-attentionEq.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='3-4'></a>\n",
    "## 3.4 Benifits\n",
    "\n",
    "<img src='../images/8-attentionBenifits.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='3-5'></a>\n",
    "## 3.5 Attention as a general DL technique\n",
    "\n",
    "- References: “Deep Learning for NLP Best Practices”, Ruder, 2017. http://ruder.io/deep-learning-nlp-best-practices/index.html#attention\n",
    "- References: “Massive Exploration of Neural Machine Translation Architectures”, Britz et al, 2017, https://arxiv.org/pdf/1703.03906.pdf\n",
    "\n",
    "<img src='../images/8-attentionGeneral.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/8-attentionGeneral2.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/8-attentionGeneral3.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='3-6'></a>\n",
    "## 3.6 Remaining problems\n",
    "\n",
    "- Further reading: “Has AI surpassed humans at translation? Not even close!” https://www.skynettoday.com/editorials/state_of_nmt\n",
    "\n",
    "MT is not a sloved problem. Many difficulties remain:\n",
    " \n",
    "- Out-of-vocabulary words\n",
    "- Domain mismatch between train and test data \n",
    "- Maintaining context over longer text\n",
    "- Low-resource language pairs\n",
    "- Using common sense is still hard\n",
    "\n",
    "<img src='../images/8-MTProblem.png' width='600' height='300'>\n",
    "\n",
    "- NMT picks up biases in training data\n",
    "\n",
    "<img src='../images/8-MTProblem2.png' width='600' height='300'>\n",
    "\n",
    "- Uninterpretable systems do strange things\n",
    "\n",
    "<img src='../images/8-MTProblem3.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/8-NMTMovesFoward.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='3-7'></a>\n",
    "## 3.7 Trend\n",
    "\n",
    "- Reference: [Edinburgh En-De WMT newstest2013 Cased BLEU; NMT 2015 from U. Montréal], http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf\n",
    "\n",
    "<img src='../images/8-MTOverTime.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/8-MTOverTime2.png' width='600' height='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cff5c",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "# 4. References\n",
    "\n",
    "- [Course website](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html)\n",
    "\n",
    "- [Lecture video](https://www.youtube.com/watch?v=XXtpJxZBa2c) \n",
    "\n",
    "- [Lecture slide](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture08-nmt.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
