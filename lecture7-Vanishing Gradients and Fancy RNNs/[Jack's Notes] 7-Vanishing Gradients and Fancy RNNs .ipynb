{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91544cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2021-08-03\n",
    "# GitHub: https://github.com/jaaack-wang \n",
    "# About: Vanishing Gradients and Fancy RNNs for Stanford CS224N- NLP with Deep Learning | Winter 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd06afa",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [1. Vanishing gradient](#1)\n",
    "    - [1.1 Problem defined and cause](#1-1)\n",
    "    - [1.2 Potential problems](#1-2)\n",
    "    - [1.3 Possible consequences of Vanishing Gradient on RNN-LM in possible scenarios](#1-3)\n",
    "- [2. Exploding gradient](#2)\n",
    "    - [2.1 Problem defined and cause](#2-1)\n",
    "    - [2.2 Potential problems](#2-2)\n",
    "    - [2.3 Solutions: gradient clipping](#2-3)\n",
    "- [3. Long Short-Term Memory (LSTM)](#3)\n",
    "    - [3.1 Description](#3-1)\n",
    "    - [3.2 Graphic representation](#3-2)\n",
    "    - [3.3 LSTM success and replacement](#3-3)\n",
    "- [4. Gated Recurrent Units (GRU)](#4)\n",
    "    - [4.1 Description](#4-1)\n",
    "    - [4.2 LSTM vs GRU](#4-2)\n",
    "- [5. General solutions to vanishing/exploding gradient](#5)\n",
    "    - [5.1 Vanishing/exploding gradient in NN](#5-1)\n",
    "    - [5.2 Residual connections](#5-2)\n",
    "    - [5.3 Dense connections](#5-3)\n",
    "    - [5.4 Highway connections](#5-4)\n",
    "- [6. Bidirectional RNNs](#6)\n",
    "    - [6.1 Motivation](#6-1)\n",
    "    - [6.2 Structure](#6-2)\n",
    "    - [6.3 Restrictions](#6-3)\n",
    "- [7. Multi-layer RNNs (Stacked)](#7)\n",
    "    - [7.1 Description](#7-1)\n",
    "    - [7.2 In practice](#7-2)\n",
    "- [8. Summary](#8)\n",
    "- [9. References](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48d26a",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1. Vanishing gradient\n",
    "\n",
    "<a name='1-1'></a>\n",
    "## 1.1 Problem defined and cause\n",
    "- Reference: [Pascanu, R; Mikolov, T; Bengio, Y. 2013. On the difficulty of training recurrent neural networks](http://proceedings.mlr.press/v28/pascanu13.pdf)\n",
    "\n",
    "<img src='../images/7-vanishingGradients.png' width='600' height='300'>\n",
    "\n",
    "**Formal proof**\n",
    "<img src='../images/7-vanishingGradients2.png' width='600' height='300'>\n",
    "\n",
    "**Exploding gradient**\n",
    "<img src='../images/7-vanishingGradients3.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='1-2'></a>\n",
    "## 1.2 Potential problems\n",
    "\n",
    "For example: $\\frac{J^{(4)}}{h^{(1)}} < \\frac{J^{(4)}}{h^{(2)}} < \\frac{J^{(4)}}{h^{(3)}} < \\frac{J^{(4)}}{h^{(3)}}$. And more loops the training goes through, the longest distance gradients (weights far away from the J) tend to be vanishing as they approach 0.\n",
    "\n",
    "<img src='../images/7-whyVGaProblem.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "\n",
    "<a name='1-3'></a>\n",
    "## 1.3 Possible consequences of Vanishing Gradient on RNN-LM in possible scenarios \n",
    "\n",
    "- Too long a sequence. This is a major problem because for a vanilla RNN (the RNN introudced so far, the state is constantly updated in each time step, which makes it impossible or hardly possible for the model to preserve long-distance dependency. In other word, the longer distance a piece of info is, the harder it will be kept in the model. \n",
    "<img src='../images/7-VGEffectonRNN-LM0.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "- Mixed info not fully learnt \n",
    "<img src='../images/7-VGEffectonRNN-LM.png' width='600' height='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeab369",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2. Exploding gradient\n",
    "\n",
    "<a name='2-1'></a>\n",
    "## 2.1 Problem defined and cause\n",
    "\n",
    "See [1.1](#1.1) **Exploding gradient** (third pic).\n",
    "\n",
    "<a name='2-2'></a>\n",
    "## 2.2 Potential problems\n",
    "\n",
    "- So-called overshotting \n",
    "\n",
    "<img src='../images/7-explodingGradient.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='2-3'></a>\n",
    "## 2.3 Solutions: gradient clipping\n",
    "\n",
    "- Reference: [Pascanu, R; Mikolov, T; Bengio, Y. 2013. On the difficulty of training recurrent neural networks]\n",
    "\n",
    "<img src='../images/7-GradientClipping.png' width='600' height='300'>\n",
    "\n",
    "- Reference: [“Deep Learning”, Goodfellow, Bengio and Courville, 2016. Chapter 10.11.1. Sequence Modeling: Recurrent and Recursive Nets](https://www.deeplearningbook.org/contents/rnn.html)\n",
    "<img src='../images/7-GradientClipping2.png' width='600' height='300'>\n",
    "\n",
    "- Can also employ more sophisticated optimizers, like Adam, Adagrad, RMSprop etc., to overcome the exploding gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c31483",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# 3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "<a name='3-1'></a>\n",
    "# 3.1 Description \n",
    "- [Hochreiter and Schmidhuber, 1997. “Long short-term memory”.](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "\n",
    "<img src='../images/7-LSTMDesc.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "- Forget gate is similar to the idea of Dropout in Deep Neural Network, an intuitive trick to reduce the risk of Vanishing Gradient.\n",
    "<img src='../images/7-LSTMDesc2.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='3-2'></a>\n",
    "# 3.2 Graphic representation \n",
    "- Reference: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "<img src='../images/7-LSTMDiag.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='3-3'></a>\n",
    "# 3.3 LSTM success and replacement\n",
    "\n",
    "- Source: \"Findings of the 2016 Conference on Machine Translation (WMT16)\", Bojar et al. 2016, http://www.statmt.org/wmt16/pdf/W16-2301.pdf \n",
    "- Source: \"Findings of the 2018 Conference on Machine Translation (WMT18)\", Bojar et al. 2018, http://www.statmt.org/wmt18/pdf/WMT028.pdf\n",
    "\n",
    "\n",
    "**Research paradigms are changing very fast!**\n",
    "<img src='../images/7-LSTMSuccess.png' width='600' height='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588390b",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "# 4. Gated Recurrent Units (GRU)\n",
    "\n",
    "<a name='4-1'></a>\n",
    "## 4.1 Description \n",
    "\n",
    "- Reference: \"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\", Cho et al. 2014, https://arxiv.org/pdf/1406.1078v3.pdf\n",
    "\n",
    "<img src='../images/7-GRUDesc.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='4-2'></a>\n",
    "## 4.2 LSTM vs GRU\n",
    "\n",
    "- Researchers have proposed many gated RNN variants, but LSTM and GRU are the most widely-used\n",
    "- The biggest difference is that **GRU is quicker** to compute and has fewer parameters\n",
    "- There is **no conclusive evidence** that one consistently performs better than the other\n",
    "- **LSTM is a good default choice** (especially if your data has particularly long dependencies, or you have lots of training data)\n",
    "- **Rule of thumb**: start with LSTM, but switch to GRU if you want something more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7e0c9",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "# 5. General solutions to vanishing/exploding gradient \n",
    "\n",
    "<a name='5-1'></a>\n",
    "## 5.1 Vanishing/exploding gradient in NN\n",
    "\n",
    "Obviously, vanishing/exploding gradient is a program that is not only relevant for RNN, but for all NN (including feed-forward and convolutional), especially deep ones. **Although, for RNN, these problems are more serious due to the design of RNN (i.e., the repeated multiplication by the same weight matrix)**. See: ”Learning Long-Term Dependencies with Gradient Descent is Difficult\", Bengio et al. 1994, http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf.\n",
    "\n",
    "**Causes and solutions:** </br>\n",
    "- Due to chain rule / choice of nonlinearity function, gradient can become vanishingly small as it backpropagates\n",
    "- Thus lower layers are learnt very slowly (hard to train)\n",
    "- Solution: lots of new deep feedforward/convolutional architectures that add more direct connections (thus allowing the gradient to flow)\n",
    "\n",
    "\n",
    "<a name='5-2'></a>\n",
    "## 5.2 Residual connections\n",
    "\n",
    "- Reference: [He et al.2015. Deep Residual Learning for Image Recognition.](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- This is a very general trick\n",
    "<img src='../images/7-ResNet.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='5-3'></a>\n",
    "## 5.3 Dense connections\n",
    "\n",
    "- Reference: ”Densely Connected Convolutional Networks\", Huang et al, 2017. https://arxiv.org/pdf/1608.06993.pdf\n",
    "- This is more specific to CNN\n",
    "<img src='../images/7-DenseNet.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='5-4'></a>\n",
    "## 5.4 Highway connections \n",
    "\n",
    "- Reference: ”Highway Networks\", Srivastava et al, 2015. https://arxiv.org/pdf/1505.00387.pdf\n",
    "- Highway connections aka “HighwayNet”\n",
    "- Similar to residual connections, but the identity connection vs the transformation layer is controlled by a dynamic gate\n",
    "- Inspired by LSTMs, but applied to deep feedforward/convolutional networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8664e89",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "# 6. Bidirectional RNNs\n",
    "\n",
    "<a name='6-1'></a>\n",
    "## 6.1 Motivation\n",
    "\n",
    "- **Contextual representation**\n",
    "- Look for both directions\n",
    "<img src='../images/7-BiRNN.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='6-2'></a>\n",
    "## 6.2 Structure \n",
    "<img src='../images/7-BiRNN2.png' width='600' height='300'>\n",
    "<img src='../images/7-BiRNN3.png' width='600' height='300'>\n",
    "<img src='../images/7-BiRNN4.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='6-3'></a>\n",
    "## 6.3 Restrictions\n",
    "<img src='../images/7-BiRNNRestriction.png' width='600' height='300'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056fabd",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "# 7. Multi-layer RNNs (Stacked)\n",
    "\n",
    "<a name='7-1'></a>\n",
    "## 7.1 Description\n",
    "- RNNs are already “deep” on one dimension (they unroll over many timesteps)\n",
    "- We can also make them “deep” in another dimension by applying multiple RNNs – this is a multi-layer RNN.\n",
    "- This allows the network to compute more complex representations\n",
    "- The lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features.\n",
    "- Multi-layer RNNs are also called stacked RNNs.\n",
    "\n",
    "\n",
    "- This can be bidirectional provided that the entire input sentence is accessible.\n",
    "<img src='../images/7-MultiLRNN.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='7-2'></a>\n",
    "## 7.2 In practice \n",
    "- Reference: “Massive Exploration of Neural Machine Translation Architecutres”, Britz et al, 2017. https://arxiv.org/pdf/1703.03906.pdf\n",
    "- Skips are usually heavily used. \n",
    "<img src='../images/7-MultiLRNNInPractice.png' width='600' height='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e236d2",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "# 8. Summary\n",
    "\n",
    "<img src='../images/7-Summary.png' width='600' height='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cff5c",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "# 9. References\n",
    "\n",
    "- [Course website](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html)\n",
    "\n",
    "- [Lecture video](https://www.youtube.com/watch?v=QEw0qEa0E50&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=7) \n",
    "\n",
    "- [Lecture slide](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture07-fancy-rnn.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
