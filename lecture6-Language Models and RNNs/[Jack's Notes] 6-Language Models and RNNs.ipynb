{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91544cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2021-10-06\n",
    "# GitHub: https://github.com/jaaack-wang \n",
    "# About: Language Models and RNNs for Stanford CS224N - NLP with Deep Learning | Winter 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd06afa",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [1. Language Modeling](#1)\n",
    "    - [1.1 Overview](#1-1)\n",
    "    - [1.2 n-gram Language Models](#1-2)\n",
    "    - [1.3 Fix-window Neural Language Model](#1-3)\n",
    "    - [1.4 Evaluation: Perplexity](#1-4)\n",
    "- [2. Recurrent Neural Networks](#2)\n",
    "    - [2.1 Overview](#2-1)\n",
    "        - [2.1.1 Basic architeture of RNN](#2-1-1)\n",
    "        - [2.1.2 Applications](#2-1-2)\n",
    "    - [2.2 RNN Language Model](#2-2)\n",
    "        - [2.2.1 Example](#2-2-1)\n",
    "        - [2.2.2 Pros and cons](#2-2-2)\n",
    "        - [2.2.3 Training](#2-2-3)\n",
    "- [3. Recap](#3)\n",
    "- [4. References](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeab369",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1. Language Modeling\n",
    "\n",
    "\n",
    "<a name='1-1'></a>\n",
    "## 1.1 Overview\n",
    "\n",
    "**Definition:**\n",
    "Language Modeling is the task of predicting what word comes next. More formally, given a sequence of words $x^{(1)}, x^{(2)}...,x^{(t)}$, compute the probability distribution of the next word $x^{(t+1)}$:\n",
    "\n",
    "$$P(x^{(t+1)}|x^{(t)},...,x^{(1)})$$\n",
    "\n",
    "where $x^{(t+1)}$ can be any word in the vocabulary $V = {w_1,...,w_{|V|}}$.\n",
    "\n",
    "This follows, we can think of a Language Model as a system that assigns probability to a piece of text, say, $x^{(1)},...,x^{(T)}$, which equals:\n",
    "\n",
    "\n",
    "$$P(x^{(1)},...,x^{(T)}) = P(x^{(1)}) \\times P(x^{(2)}|x^{(1)}) \\times P(x^{(T)}|x^{(T-1)},...,x^{(1)}) = \\prod P(x^{(t)}|x^{(t-1)},...,x^{(1)}) \\tag{1-1}$$\n",
    "\n",
    "\n",
    "<a name='1-2'></a>\n",
    "## 1.2 n-gram Language Models\n",
    "\n",
    "\n",
    "- Idea: Collection statistics about how frequent different n-grams are, and use these to predict next word (see formula 1-1 above).\n",
    "- Assumption: $x^{(t+1)}$ depends only on the preceding $n-1$ words (this is a very simplistic view).\n",
    "\n",
    "**Problems:**\n",
    "- Sparsity problem: not much granularity in the probability distribution\n",
    "    - Less depedent on contexts, especially complex one. \n",
    "    - Unseen n-gram/events in the training set. (Smoothing + backoff)\n",
    "- Storage (With n increasing, the storage also increases drastically, see [my ngram experiments with Chinese](https://github.com/jaaack-wang/ChineseNgrams)) \n",
    "\n",
    "**Example (text generation):**\n",
    "<img src='../images/6-ngramTextG.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='1-3'></a>\n",
    "## 1.3 Fix-window Neural Language Model\n",
    "\n",
    "<img src='../images/6-neuralLM.png' width='600' height='300'>\n",
    "\n",
    "Improvements & Problems:\n",
    "\n",
    "- Also, this can be very expensive to compute when the vocab size is extremely large. \n",
    "\n",
    "<img src='../images/6-fixNLM-ImprProblm.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='1-4'></a>\n",
    "## 1.4 Evaluation: Perplexity\n",
    "\n",
    "<img src='../images/6-perplexity.png' width='600' height='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f9b448",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2. Recurrent Neural Networks\n",
    "\n",
    "<a name='2-1'></a>\n",
    "## 2.1 Overview\n",
    "\n",
    "<a name='2-1-1'></a>\n",
    "### 2.1.1 Basic architeture of RNN\n",
    "\n",
    "<img src='../images/6-RRNArch.png' width='600' height='300'>\n",
    "\n",
    "<a name='2-1-2'></a>\n",
    "### 2.1.2 Applications\n",
    "\n",
    "- Language Model\n",
    "- Part of sppech\n",
    "- Sentiment classification\n",
    "- Question answering\n",
    "- Speech recognition\n",
    "- Machine Translation\n",
    "\n",
    "\n",
    "<a name='2-2'></a>\n",
    "## 2.2 RNN Language Model\n",
    "\n",
    "<a name='2-2-1'></a>\n",
    "### 2.2.1 Example\n",
    "Model example\n",
    "<img src='../images/6-RRNLM.png' width='600' height='300'>\n",
    "\n",
    "Application example:\n",
    "<img src='../images/6-RRNexample.png' width='600' height='300'>\n",
    "\n",
    "<a name='2-2-2'></a>\n",
    "### 2.2.2 Pros and cons\n",
    "\n",
    "<img src='../images/6-RRNProsCons.png' width='600' height='300'>\n",
    "\n",
    "<a name='2-2-3'></a>\n",
    "### 2.2.3 Training\n",
    "<img src='../images/6-tranRRNLM.png' width='600' height='300'>\n",
    "<img src='../images/6-tranRRNLM2.png' width='600' height='300'>\n",
    "<img src='../images/6-RRNbackpro.png' width='600' height='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2b39c",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# 3. Recap\n",
    "\n",
    "<img src='../images/6-Recap.png' width='600' height='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cff5c",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "# 4. References\n",
    "\n",
    "- [Course website](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html)\n",
    "\n",
    "- [Lecture video](https://youtu.be/iWea12EAu6U) \n",
    "\n",
    "- [Lecture slide](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture06-rnnlm.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
