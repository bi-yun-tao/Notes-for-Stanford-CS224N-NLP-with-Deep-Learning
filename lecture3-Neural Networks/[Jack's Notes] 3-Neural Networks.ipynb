{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0fed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2021-08-03\n",
    "# GitHub: https://github.com/jaaack-wang \n",
    "# About: Word Vectors and Word Senses for Stanford CS224N- NLP with Deep Learning | Winter 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f832fc",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [1. Casual takeaways](#1)\n",
    "- [2. Classification review and notation](#2)\n",
    "    - [2.1 Inputs and labels](#2-1)\n",
    "    - [2.2 Output models](#2-2)\n",
    "    - [2.3 Objective/loss function and cross entropy](#2-3)\n",
    "    - [2.4 Traditional ML classifier versus Neural Network Classifier](#2-4)\n",
    "- [3. Neural Network Basics](#3)\n",
    "    - [3.1 Neuron](#3-1)\n",
    "    - [3.2 What an artificial neuron can do](#3-2)\n",
    "    - [3.3 Matrix notation for a layer](#3-3)\n",
    "- [4. Named Entity Recognition (NER)](#4)\n",
    "    - [4.1 NER task](#4-1)\n",
    "    - [4.2 Simple NER training methods](#4-2)\n",
    "        - [4.2.1 Binary classification with averaged context words vectors](#4-2-1)\n",
    "        - [4.2.2 Window classification using multi-class softmax classifier](#4-2-2)\n",
    "        - [4.2.3 Binary classification with unnormalized scores using shallow neural network](#4-2-3)\n",
    "    - [4.3 Challenges for NER](#4-3)\n",
    "- [5. Gradients computation](#5)\n",
    "    - [5.1 Partial derivatives and gradients](#5-1)\n",
    "        - [5.1.1 Simple case: placed in a vector](#5-1-1)\n",
    "        - [5.1.2 A more complex case: placed in a matrix -- Jacobian Matrix](#5-1-2)\n",
    "    - [5.2 Multivariate calculus](#5-2)\n",
    "        - [5.2.1 The chain rule](#5-2-1)\n",
    "        - [5.2.2 Partial derivatives computation in Jacobian Matrix](#5-2-2)\n",
    "        - [5.2.3 Shape convention](#5-2-3)\n",
    "- [6. References](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d530e9",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1. Casual takeaways\n",
    "\n",
    "- This lecture is mostly about the neural net fundamentals.\n",
    "\n",
    "\n",
    "- The slides for [this lecture video](https://www.youtube.com/watch?v=8CWyBNX6eDo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=3) are in both the [Word Vectors 2 and Word Window Classification slide starting page 42](http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture02-wordvecs2.pdf) and the [Backprop and Neural Networks slide](http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture03-neuralnets.pdf). The original slide in the video has been restructured on the course video (which holds the 2021 version of the course).\n",
    "\n",
    "\n",
    "- Cross-entropy loss is more convenient than negative log probability in terms of computation. \n",
    "\n",
    "\n",
    "- Manning said \"baby information\" (refer to cross entry among other things) \"is about the amount of information theory\" he knows. Interesting!\n",
    "\n",
    "\n",
    "- Problems with the labelled data: 1. we do not have the labelled data; (labelled data are expensive and scarce) 2. the labels might not be easy for human raters to determine. \n",
    "\n",
    "\n",
    "- In general, classifying single words is rarely done\n",
    "\n",
    "- There is a lot of inconsistency as to how people represent matrix calculus. Differnet conventions: numerator convention, denominator convention, shape convention. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fc2f8",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2. Classification review and notation\n",
    "\n",
    "<a name='2-1'></a>\n",
    "## 2.1 Inputs and labels\n",
    "\n",
    "\n",
    "For any **supervised** classification tasks, you have inputs and expected outputs (labels), which can be denoted by \n",
    "\n",
    "$$\\{x_{i}, y_{i}\\}_{i=1}^{N}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x_{i}$ are inputs, e.g., words (indices or vectors!), sentences, documents, etc.\n",
    "- $y_{i}$ are labels (one of C classes) we try to predict, for example:\n",
    "    - classes: sentiment (+/–), named entities, buy/sell decision\n",
    "    - other words (such as in word2vec algorithms\n",
    "    - multi-word sequences (e.g., Recurrent Neural Network, Machine Translation)\n",
    "- $N$ is the sample size\n",
    "\n",
    "<br>\n",
    "\n",
    "<a name='2-2'></a>\n",
    "## 2.2 Output models\n",
    "\n",
    "\n",
    "\n",
    "<font color='blue'>\\- Sigmoid function for binary classification:</font>\n",
    "\n",
    "$$p(y|x) = \\frac{1}{1 + exp(-\\mathbf{w_y \\cdot x})} \\tag{1}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{w_y}$ is the weight vector used to predict $y$ (one of two classes): **in the right prediction cases**, if $y = 1$, then $p(y|x)$ should generally be greater than 0.5; if $y = 0$, then $p(y|x)$ should generally be less than 0.5.\n",
    "- Manning does not provide this formula in the slide. This formula is written according to the way he writes the softmax formula down below.\n",
    "- In this computation, $\\mathbf{x}$ should be verctorized. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='blue'>\\- Softmax function for multi-class classification:</font>\n",
    "\n",
    "$$p(y|x) = \\frac{exp(\\mathbf{W_{y} \\cdot x})}{\\sum_{c=1}^{C} exp(\\mathbf{W_{c} \\cdot x})} \\tag{2}$$\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{W_y}$ is the $y_{th}$ row of the weight matirx $W$ used to predict $y$ (one of many classes): **in the right prediction cases**, the $\\mathbf{W_y}$ should give the $y$ class the biggest share of probability among other classes (not necesssarily larger than 0.5).\n",
    "\n",
    "\n",
    "- $\\mathbf{W_y} \\in \\mathbb{R}^{C \\times d}$ where $C$ denotes the total number of classes we have for prediction and $d$ denotes the dimension for each weight vector in the matrix. \n",
    "\n",
    "\n",
    "- $\\sum_{c=1}^{C} exp(\\mathbf{W_{c} \\cdot x})$ normalizes the numerator, converts it into a probablity distribution and makes the sum of the probabilities of all classes equals 1.\n",
    "\n",
    "\n",
    "- In this computation, $\\mathbf{x}$ should also be verctorized. \n",
    "\n",
    "\n",
    "- To simplify the notation, we will use $f_{y}$ to denote $\\mathbf{W_{y} \\cdot x}$ and $f_{c}$ to denote $\\mathbf{W_{c} \\cdot x})$ so that (2) can be rewritten as:\n",
    "\n",
    "$$p(y|x) = \\frac{exp(f_y)}{\\sum_{c=1}^{C} exp(f_c)} \\tag{2}$$\n",
    "\n",
    "\n",
    "\n",
    "<font color='blue'>Notes</font>\n",
    "\n",
    "- We can also apply the sigmoid function multiply $C (> 2)$ times to get normalized probablity distribution for multi-classes whose sum equals 1:\n",
    "    - In each iteration of the sigmoid function, we can take the class $c_i$ we want to predict as 1 and the rest 0 and run sigmoid function;\n",
    "    - Then, we take the probability in each iteration for the class $c_i$ as the numerator and their sum as the denominator; \n",
    "    - Finally, we divide the numerator by the denominator and get the normalized probablity distribution for all classes. \n",
    "    - As a result of this multi-class logistic regression training, we will also have a weight matrix $\\mathbf{W_y} \\in \\mathbb{R}^{C \\times d}$.\n",
    "    - Mathematically, softmax is a simplied version of this one-versus-all multi-class logistic regression.\n",
    "    \n",
    "\n",
    "\n",
    "<a name='2-3'></a>\n",
    "## 2.3 Objective/loss function and cross entropy\n",
    "\n",
    "For each training example $(x, y)$, we want to **maximize the probability of the correct class** y. We usually do this by **minizing the negative log probability of that class**:\n",
    "\n",
    "$$\\log p(y|x) - \\log( \\frac{exp(f_y)}{\\sum_{c=1}^{C} exp(f_c)} ) \\longrightarrow 0$$\n",
    "\n",
    "\n",
    "However, a more convenient and common way to compute the loss function for both logsitic regression and softmax is **cross entroy**: \n",
    "\n",
    "\n",
    "$$H(p, q) = - \\sum_{c=1}^{C}p(c)\\log q(c) \\tag{3}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $p$ is the true probability distribution \n",
    "- $q$ is the computed model probability \n",
    "- $H(p, q)$ denotes the cross entropy loss between p and q\n",
    "\n",
    "<br>\n",
    "\n",
    "**The reason why this can work is that: $p(c)$ will output 1 when $c$ is the true class to predict and 0 everywhere else such that the only term left will be the negative log probability of the true class.** In real computation, we usually verctorize both $p$ and $q$ as $\\mathbf{y}$ (a one-hot vector) and $\\hat y$ (the probability vector for all classes) so that (3) can be rewritten as:\n",
    "\n",
    "$$H(\\mathbf{y, \\hat y}) = - \\mathbf{y \\cdot \\log \\hat y} \\tag{4}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "As we want to maximize the performance of our model over the entire dataset $\\{x_{i}, y_{i}\\}_{i=1}^{N}$, we will average the loss function illustrated in (3) and (4) accordingly: \n",
    "\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} - \\log(\\frac{exp({f_{y_i})}}{\\sum_{c=1}^{C}exp({f_c})}) \\tag{5}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='blue'>Another question: why the negative log probability?</font>\n",
    "\n",
    "- The negative log probability is actually closely related to the concept of **odd** in probability theory because it is just the logged odd.\n",
    "    - to put it simply, the odd of $p$ to $q$ is just the ratio between them: $\\frac{p}{q}$\n",
    "    - if $p \\approx q$, then $\\frac{p}{q} \\longrightarrow 1$\n",
    "    - take the log of $\\frac{p}{q}$, we get $\\log \\frac{p}{q} = \\log q - \\log q$. As $\\frac{p}{q} \\longrightarrow 1$, $\\log q - \\log q \\longrightarrow 0$.\n",
    "    - set $q = 1$, then $\\log p = 0$, $\\log q - \\log q$ can then be simplified as $1 \\times (0 - \\log q) = q \\log q$. \n",
    "\n",
    "\n",
    "- Geometrically, $-\\log x \\longrightarrow 0$ as $x \\longrightarrow 1$ (predicting in the right direction) and $-\\log x \\longrightarrow +\\infty$ as $x \\longrightarrow 0$ (predicting in the wrong direction). You can see the visualization of the negative log probability below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5ab444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'cross entropy loss illustration')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(1e-10, 1, 100)\n",
    "plt.plot(x, -np.log(x), label='negative log probability')\n",
    "plt.legend()\n",
    "plt.grid(b=True)\n",
    "plt.title('cross entropy loss illustration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef07ed2",
   "metadata": {},
   "source": [
    "<a name='2-4'></a>\n",
    "## 2.4 Traditional ML classifier versus Neural Network Classifier\n",
    "\n",
    "In short, compared to the neural network classifier, the main problem with traditional ML classifier is it only has one layer of weights $W$ to train and the decision boundary is less flexible as a result of that (**not necessary alway linear if we can add polynomial terms**). \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='../images/3-traditional-ml-optimization.png' width='600' height='300'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='../images/3-traditional-ml-optimization-problem.png' width='600' height='300'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='../images/3-neural-network-classifier-wins.png' width='600' height='300'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdff09",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# 3. Neural Network Basics\n",
    "\n",
    "<a name='3-1'></a>\n",
    "## 3.1 Neuron\n",
    "\n",
    "Artificial neural network takes inspiration from the human neuron, which looks as follows:\n",
    "\n",
    "<img src='../images/3-neuron-real.png' width='600' height='300'>\n",
    "\n",
    "<br> \n",
    "\n",
    "The artificial neuron:\n",
    "<img src='../images/3-neuron-artificial.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='blue'>Notes</font>\n",
    "\n",
    "However, mathematically, I find it much easier to think of artificial neural network just a complicated composite function where the inputs for an (more) inner function comes from the outputs of an (more) outer function. Concretely, for our dataset $\\{x_{i}, y_{i}\\}_{i=1}^{N}$, we input $\\{x_{i}\\}_{i=1}^{N}$ and we get $\\{y_{i}\\}_{i=1}^{N}$. In between, the inputs $\\{x_{i}\\}_{i=1}^{N}$ can go through the following process before they get outputted as $\\{\\hat y_{i}\\}_{i=1}^{N}$ (the following $\\mathbf{x}$ and $\\mathbf{y}$ are vectorized:\n",
    "\n",
    "$$\\mathbf{x} \\longrightarrow \\mathbf{A^{(1)}} = f^{(1)}(\\mathbf{x}) \\longrightarrow \\mathbf{A^{(2)}} = f^{(2)}(\\mathbf{A^{(1)}}) \\rightarrow ... \\rightarrow \\mathbf{A^{(N-1)}} = f^{(N-1)}(\\mathbf{A^{(N-2)}}) \\longrightarrow \\mathbf{\\hat y} = f^{(N)}(\\mathbf{A^{(N-1)}}) \\longrightarrow \\mathbf{\\hat y} $$\n",
    "\n",
    "Or:\n",
    "\n",
    "$$\\mathbf{\\hat y} = f^{(N)}(f^{(N-1)}(...\\longrightarrow f^{(2)}(f^{(1)}\\mathbf(x))))$$\n",
    "\n",
    "\n",
    "where $f = \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}$, $\\mathbf{W}$ is the weight and $\\mathbf{b}$ the bias term, both vectorized. $f$ is often called activation function and the interval between functions $f$ are known as layers. **Also note that the $f$ does not necessarily remain the same across different layers.**\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='blue'>**Neural network is just a mathematical trick, which is made possible by the availability of big data and the increasing computing power. Except that, there is nothing mysterious about it.**</font>  \n",
    "\n",
    "\n",
    "<a name='3-2'></a>\n",
    "## 3.2 What an artificial neuron can do\n",
    "\n",
    "- As a binary logistic regression unit:\n",
    "\n",
    "<img src='../images/3-neuron-as-binary-clf.png' width='600' height='300'>\n",
    "\n",
    "<br>\n",
    "\n",
    "- Construct a shallow neural network:\n",
    "\n",
    "<img src='../images/3-neuron-to-shallow-nn.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "\n",
    "- Construct a more complicated or deeper neural network:\n",
    "\n",
    "<img src='../images/3-neuron-to-deeper-nn.png' width='600' height='300'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='../images/3-neuron-to-deeper-nn-multilayers.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "- keep going ...\n",
    "\n",
    "- Increases the power of neural network to do classification (much more flexible)\n",
    "\n",
    "<img src='../images/3-power-of-nn-in-clf.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a name='3-3'></a>\n",
    "## 3.3 Matrix notation for a layer\n",
    "\n",
    "<img src='../images/3-matrix-notation-for-layers.png' width='600' height='300'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a98aa",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "# 4. Named Entity Recognition (NER)\n",
    "\n",
    "\n",
    "<a name='4-1'></a>\n",
    "## 4.1 NER task\n",
    "\n",
    "<img src='../images/3-ner-about.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='4-2'></a>\n",
    "## 4.2 Simple NER training methods\n",
    "\n",
    "<br>\n",
    "\n",
    "<a name='4-2-1'></a>\n",
    "### 4.2.1 Binary classification with averaged context words vectors\n",
    "\n",
    "- Recall the notes for lecture 2 where we learned two word2vec models. This is similar to the Continous Bag of Words model in that both use the context words to learn the center words.\n",
    "\n",
    "\n",
    "- For the conetxt words, we can average them in order to get the averaged context vector to speed up training\n",
    "\n",
    "\n",
    "- In general, classifying single words is rarely done\n",
    "\n",
    "<img src='../images/3-simply-ner-binary.png' width='600' height='300'>\n",
    "<img src='../images/3-simply-ner-binary-2.png' width='600' height='300'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<a name='4-2-2'></a>\n",
    "### 4.2.2 Window classification using multi-class softmax classifier\n",
    "\n",
    "- unlike the previous, when using softmax, we will compute the probability distribution for all words in the window, not just the center word\n",
    "\n",
    "\n",
    "- the goal is the make sure that the center word gets the highest probability score\n",
    "\n",
    "\n",
    "- please note that, the following two slides are only available in the video, not on the website\n",
    "\n",
    "<img src='../images/3-ner-clf-softmax.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/3-ner-clf-softmax2.png' width='600' height='300'>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<a name='4-2-3'></a>\n",
    "### 4.2.3 Binary classification with unnormalized scores using shallow neural network\n",
    "\n",
    "- Related papers: [Collobert & Weston. 2008. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning](https://thetalkingmachines.com/sites/default/files/2018-12/unified_nlp.pdf)\n",
    "\n",
    "\n",
    "- Recall the notes for lecture 2 where we learned two word2vec models. This is similar to the Skip-gram model in that both have positive and negative samples. The difference is this mehod predicts the whole window as to whether there is a true center word in it. \n",
    "\n",
    "\n",
    "- the following five slides are only available in the video, not on the website\n",
    "\n",
    "<img src='../images/3-simply-ner-binary-unnorm.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/3-simply-ner-binary-unnorm2.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/3-simply-ner-binary-unnorm3.jpg' width='600' height='300'>\n",
    "\n",
    "<img src='../images/3-simply-ner-binary-unnorm4.png' width='600' height='300'>\n",
    "\n",
    "<img src='../images/3-simply-ner-binary-unnorm5.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a name='4-3'></a>\n",
    "## 4.3 Challenges for NER\n",
    "\n",
    "- Ambiguity of the entity's boundary\n",
    "- Ambiguity of entity itself\n",
    "- Ambiguity of entity class (contextual)\n",
    "\n",
    "<img src='../images/3-challenges-for-ner.png' width='700' height='300'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94d63c",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "# 5. Gradients computation\n",
    "\n",
    "<font color='blue'>Notes: </font>Both the slide and the video are very casual about this part. I would recommend people take the [Introduction to Calculus](https://www.coursera.org/learn/introduction-to-calculus) course on Coursera to get started on calculus. That course will help beginners gain intuitions about calculus, although multivariate calculus and concepts like partial derivatives are not explored there. \n",
    "\n",
    "<a name='5-1'></a>\n",
    "## 5.1 Partial derivatives and gradients\n",
    "\n",
    "<a name='5-1-1'></a>\n",
    "### 5.1.1 Simple case: placed in a vector\n",
    "\n",
    "<font color='blue'>Notes: </font> The concept of partial derivatives is very important in deep learning as we will want to optimize the weights with regard to different variables. \n",
    "\n",
    "- $\\partial$ stands for partial derivatives whereas $d$ stands for derivatives (non partial)\n",
    "- When deriving partial derivatives, we usually take the variable of interest as variables and other non-interested variables as constant ($d=0$), similar to what we do in single variable calculus. For example, in $f = w_1 x_1 + w_2 x_2$, $\\frac{\\partial f}{\\partial x_1} = w_1$, $\\frac{\\partial f}{\\partial x_2} = w_2$. \n",
    "- A gradient is a vector of partial derivatives with respect to each variable. For example, in $f = w_1 x_1 + w_2 x_2$, $\\frac{\\partial f}{\\partial x} = [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2} ]= [w_1, w_2]$.\n",
    "\n",
    "<img src='../images/3-partial-derivatives.png' width='600' height='300'>\n",
    "\n",
    "<a name='5-1-2'></a>\n",
    "### 5.1.2 A more complex case: placed in a matrix -- Jacobian Matrix \n",
    "\n",
    "- This can be seen as placing $m$ row of gradients (partial derivatives stored as a vector, as in the simple case) to make a matrix. \n",
    "- Vectorization of gradients help speed up computation. \n",
    "- Jacobian Matrix is widely used in neural network backpropagation. \n",
    "\n",
    "\n",
    "<img src='../images/3-jacobian-matrix.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "<a name='5-2'></a>\n",
    "## 5.2 Multivariate calculus\n",
    "\n",
    "<a name='5-2-1'></a>\n",
    "### 5.2.1 The chain rule\n",
    "\n",
    "- General form of the chain rule: suppose $z = f(y)$, $y = f(x)$, $\\frac{d z}{d x} = \\frac{d z}{d y} \\frac{d y}{d x} $. Remember to change the $d$ into $\\partial$ if the variable to differentiate is only one of the many variables in a function.  \n",
    "- Remember that $\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}} = \\sum_{i=1}^{m} \\sum_{j=1}^{n}\\frac{f_{i}}{x_j}$ (I rewrite the formula given in the figure above to make it easier to understand),we take the partial derivatives variable by variable for each one of the functions $f$. So be careful with the order!\n",
    "- Similarly, for the following chain rule example in Jacobian matrix, $\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} = \\sum_{i=1}^{m} \\sum_{j=1}^{n}\\frac{h_{i}}{z_j}\\frac{z_{i}}{x_j}$. Therefore, in your result you should get a m by n Jacobian Matrix. (In the following slides, Manning make m = n, then the result becomes n by n dimensional)\n",
    "\n",
    "<img src='../images/3-chain-rule.png' width='600' height='300'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<a name='5-2-2'></a>\n",
    "### 5.2.2 Partial derivatives computation in Jacobian Matrix\n",
    "As I said in 5.1.1, when we take the partial derivative of variable of our interest, we only take that variable as variable and other variables as constants (meaning if we differentiate them by the variable of our interst, then the result is 0 because they are taken as constants), just like what we will do in single variable calculus. Formally, this can be expressed as the following:\n",
    "\n",
    "$$(\\frac{\\partial h}{\\partial z})_{ij} = \\frac{\\partial h_i}{\\partial z_j} = \\frac{\\partial}{\\partial z} h(z_i) $$\n",
    "\n",
    " $$ =  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      f'(z_i)  & if \\;\\; i = j \\\\\n",
    "      0  & if \\;\\; otherwise \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Examples**\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{Wx + b} = \\mathbf{W}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{b}} \\mathbf{Wx + b} = \\mathbf{I}$$\n",
    "\n",
    "where $\\mathbf{I}$ is Identity matrix. \n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{u}} \\mathbf{u^{T}h} = \\mathbf{h^{T}}$$ \n",
    "\n",
    "however, by “shape convention” (see below), we usually write $\\mathbf{h^{T}}$ as $\\mathbf{h}$ in practice (because $\\mathbf{h}$ is a row vector and we want to update $\\mathbf{h}$ here. It is easier to get a row vector output to update another row vector by element-wise subtraction).\n",
    "\n",
    "\n",
    "<a name='5-2-3'></a>\n",
    "### 5.2.3 Shape convention\n",
    "\n",
    "- Shape convention: the shape of the gradient is the shape of the parameters!\n",
    "- This makes it a whole lot easier to update the parameters. \n",
    "- We will take more about the $\\delta$, the error term for the local layer in backpropagation. Do not worry about it if you do not quite understand at this point. Basically, to optimize our neural network, we want to minimize the error as much as possible. In backpropagation, the first level of error is at the output level and then propagated to each layer backward (excluding the input layer). $\\delta$ stands for the error term, which will be reused in computing the gradients.\n",
    "\n",
    "<img src='../images/3-shape-convention.png' width='600' height='300'>\n",
    "<img src='../images/3-shape-convention2.png' width='600' height='300'>\n",
    "<img src='../images/3-shape-convention3.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Now compute the partial derivatives for 4.2.3 yourself!**</font> (you can check the answer in the slide two in the [references](#6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03340a",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "# 6. References\n",
    "\n",
    "- [Course website](http://web.stanford.edu/class/cs224n/index.html)\n",
    "\n",
    "- [Lecture video](https://www.youtube.com/watch?v=8CWyBNX6eDo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=3) \n",
    "\n",
    "- [Slide one: Word Vectors 2 and Word Window Classification slide (starting page 42)](http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture02-wordvecs2.pdf) \n",
    "- [Slide two: Backprop and Neural Networks slide](http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture03-neuralnets.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
